name: CI

on:
  pull_request:

permissions:
  contents: read
  pull-requests: write

jobs:
  format:
    name: Check Formatting (cargo fmt)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt

      - uses: swatinem/rust-cache@v2

      - name: Check formatting
        run: cargo fmt --check

  clippy:
    name: Lint (cargo clippy)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy

      - uses: swatinem/rust-cache@v2

      - name: Run Clippy (deny warnings)
        run: cargo clippy -- -D warnings

  unit-tests:
    name: Unit Tests & Coverage
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable
        with:
          components: llvm-tools-preview

      - uses: swatinem/rust-cache@v2

      - uses: taiki-e/install-action@cargo-llvm-cov

      - name: Run unit tests with coverage
        id: tests
        run: |
          # Run tests with coverage, capturing both test output and coverage data
          TEST_OUTPUT=$(cargo llvm-cov --bin log-workout --lcov --output-path lcov.info 2>&1)
          echo "$TEST_OUTPUT"
          # Strip Cargo compilation/progress lines; keep only test results
          CLEAN_OUTPUT=$(echo "$TEST_OUTPUT" | grep -Ev '^\s*(Compiling|Checking|Finished|Downloaded|Downloading|Updating|Locking|Fresh)')
          echo "test_output<<EOF" >> "$GITHUB_OUTPUT"
          echo "$CLEAN_OUTPUT" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

      - name: Generate coverage summary
        id: cov
        run: |
          SUMMARY=$(cargo llvm-cov report --summary-only 2>&1)
          echo "summary<<EOF" >> "$GITHUB_OUTPUT"
          echo "$SUMMARY" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

      - name: Check coverage gate for tested files
        id: gate
        run: |
          # Files that contain unit tests (have #[cfg(test)] blocks)
          TESTED_FILES=$(grep -rl '#\[cfg(test)\]' src/ | sort)
          echo "Tested files:"
          echo "$TESTED_FILES"

          FAIL=0
          GATE_REPORT=""
          for f in $TESTED_FILES; do
            # Extract line coverage for this file from lcov.info
            IN_FILE=0
            LH=0
            LF=0
            while IFS= read -r line; do
              case "$line" in
                SF:*"$f") IN_FILE=1 ;;
                SF:*) IN_FILE=0 ;;
                LH:*) [ "$IN_FILE" = 1 ] && LH="${line#LH:}" ;;
                LF:*) [ "$IN_FILE" = 1 ] && LF="${line#LF:}" ;;
              esac
            done < lcov.info

            if [ "$LF" -gt 0 ]; then
              PCT=$((LH * 100 / LF))
            else
              PCT=0
            fi

            STATUS="âœ…"
            if [ "$PCT" -lt 90 ]; then
              STATUS="âŒ"
              FAIL=1
            fi
            GATE_REPORT="${GATE_REPORT}| ${f} | ${LH}/${LF} | ${PCT}% | ${STATUS} |\n"
            echo "$f: $LH/$LF lines covered ($PCT%)"
          done

          echo "gate_report<<EOF" >> "$GITHUB_OUTPUT"
          echo -e "$GATE_REPORT" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"
          echo "gate_failed=$FAIL" >> "$GITHUB_OUTPUT"

      - name: Post unit-test coverage comment
        uses: actions/github-script@v7
        env:
          TEST_OUTPUT: ${{ steps.tests.outputs.test_output }}
          SUMMARY: ${{ steps.cov.outputs.summary }}
          GATE_REPORT: ${{ steps.gate.outputs.gate_report }}
          GATE_FAILED: ${{ steps.gate.outputs.gate_failed }}
        with:
          script: |
            // Access variables via process.env
            const testOutput = process.env.TEST_OUTPUT;
            const summary = process.env.SUMMARY;
            const gateReport = process.env.GATE_REPORT;
            const gateFailed = process.env.GATE_FAILED === '1';
            const gateStatus = gateFailed 
              ? 'âŒ **FAILED** â€” tested files must have â‰¥ 90% coverage' 
              : 'âœ… **PASSED** â€” all tested files have â‰¥ 90% coverage';
            const tag = '';
            const body = `${tag}
            ## ðŸ§ª Unit Tests & Coverage

            ### Test Results
            \`\`\`
            ${testOutput}
            \`\`\`

            ### Coverage Summary
            \`\`\`
            ${summary}
            \`\`\`

            ### Coverage Gate ${gateStatus}
            | File | Lines Hit/Total | Coverage | Status |
            |------|----------------|----------|--------|
            ${gateReport}`;
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            const existing = comments.find(c => c.body.includes(tag));
            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body,
              });
            }

      - name: Upload lcov report
        uses: actions/upload-artifact@v4
        with:
          name: unit-coverage-lcov
          path: lcov.info
          retention-days: 7

      - name: Fail if coverage gate not met
        if: steps.gate.outputs.gate_failed == '1'
        run: |
          echo "::error::Coverage gate failed: one or more tested files have less than 90% line coverage."
          exit 1

  e2e:
    name: E2E Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable
        with:
          target: wasm32-unknown-unknown

      - uses: swatinem/rust-cache@v2

      - name: Install Dioxus CLI
        uses: taiki-e/install-action@v2
        with:
          tool: dioxus-cli

      - uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "npm"

      - run: npm ci

      - name: Install Playwright browsers
        run: npx playwright install --with-deps chromium

      - name: Run E2E tests
        id: e2e
        run: npx playwright test
        continue-on-error: true

      - name: Upload screenshots to Cloudinary
        if: steps.e2e.outcome == 'failure'
        id: cloudinary
        env:
          CLOUDINARY_CLOUD_NAME: ${{ secrets.CLOUDINARY_CLOUD_NAME }}
          CLOUDINARY_UPLOAD_PRESET: ${{ secrets.CLOUDINARY_UPLOAD_PRESET }}
        run: |
          CLOUDINARY_URLS=""
          if [ -n "$CLOUDINARY_CLOUD_NAME" ] && [ -n "$CLOUDINARY_UPLOAD_PRESET" ] && [ -d test-results ]; then
            while IFS= read -r -d '' f; do
              RESPONSE=$(curl -sf --location \
                "https://api.cloudinary.com/v1_1/${CLOUDINARY_CLOUD_NAME}/image/upload" \
                --form "file=@$f" \
                --form "upload_preset=${CLOUDINARY_UPLOAD_PRESET}" \
                || echo '{"error":{"message":"curl failed"}}')
              if echo "$RESPONSE" | jq -e '.error' > /dev/null 2>&1; then
                echo "Warning: failed to upload $f to Cloudinary: $(echo "$RESPONSE" | jq -r '.error.message')"
                continue
              fi
              URL=$(echo "$RESPONSE" | jq -r '.secure_url // empty')
              if [ -n "$URL" ]; then
                TITLE=$(basename "$f" .png)
                CLOUDINARY_URLS="${CLOUDINARY_URLS}![${TITLE}](${URL})\n"
              fi
            done < <(find test-results -name "*.png" -print0)
          fi
          echo "cloudinary_urls<<EOF" >> "$GITHUB_OUTPUT"
          echo -e "$CLOUDINARY_URLS" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

      - name: Post failure-screenshots comment
        if: steps.e2e.outcome == 'failure'
        uses: actions/github-script@v7
        env:
          CLOUDINARY_URLS: ${{ steps.cloudinary.outputs.cloudinary_urls }}
        with:
          script: |
            const tag = '<!-- e2e-screenshots -->';
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            const cloudinaryUrls = process.env.CLOUDINARY_URLS || '';
            let body = `${tag}\n## ðŸ“¸ E2E Test Failures\n\n`;
            if (cloudinaryUrls.trim()) {
              body += cloudinaryUrls;
            } else {
              body += `Test failures were detected. Screenshots are available in the **Artifacts** section of the [Action Run](${runUrl}).\n`;
            }
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            const existing = comments.find(c => c.body.includes(tag));
            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body,
              });
            }

      - name: Fail if any E2E test failed
        if: steps.e2e.outcome == 'failure'
        run: exit 1

  e2e-android:
    name: Android E2E Tests (Maestro)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: "17"
          distribution: temurin

      # Build for x86_64 so the APK runs on the CI emulator.
      # Omit --release so Rust compiles with debug_assertions=true. wry 0.53+
      # defaults devtools=true when debug_assertions is on, which calls
      # WebView.setWebContentsDebuggingEnabled(true) â€” enabling the Chrome
      # DevTools Protocol (CDP) socket that Maestro uses to read WebView content.
      - name: Install x86_64 Android Rust target
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: x86_64-linux-android

      - uses: swatinem/rust-cache@v2

      - name: Install Dioxus CLI
        uses: taiki-e/install-action@v2
        with:
          tool: dioxus-cli

      - name: Build x86_64 APK for emulator testing
        run: >-
          dx build
          --platform android
          --target x86_64-linux-android
          --no-default-features
          --features mobile-platform

      - name: Locate x86_64 APK
        id: apk
        run: |
          SRC=$(find target/dx -name "*.apk" -maxdepth 10 | head -1)
          [ -n "$SRC" ] || { echo "::error::No APK found under target/dx"; exit 1; }
          echo "Found APK: $SRC"
          echo "path=$(realpath "$SRC")" >> "$GITHUB_OUTPUT"

      # Enable KVM for hardware-accelerated Android emulation on ubuntu-latest.
      - name: Enable KVM
        run: |
          echo 'KERNEL=="kvm", GROUP="kvm", MODE="0666", OPTIONS+="static_node=kvm"' | sudo tee /etc/udev/rules.d/99-kvm4all.rules
          sudo udevadm control --reload-rules
          sudo udevadm trigger --name-match=kvm

      - name: Install Maestro
        run: |
          curl -Ls "https://get.maestro.mobile.dev" | bash
          echo "$HOME/.maestro/bin" >> "$GITHUB_PATH"

      - name: Run Maestro flows on Android emulator
        id: maestro
        # continue-on-error so the artifact upload step below always runs
        continue-on-error: true
        uses: reactivecircus/android-emulator-runner@v2
        with:
          api-level: 33
          arch: x86_64
          # google_apis provides the Google Chrome WebView (chromium-based) with
          # full Chrome DevTools Protocol (CDP) support. The default AOSP system
          # image ships a stripped-down WebView that Maestro cannot inspect via
          # CDP, causing all text assertions to time out.
          target: google_apis
          script: |
            adb install "${{ steps.apk.outputs.path }}"
            MAESTRO_CLI_NO_ANALYTICS=1 maestro test maestro/

      - name: Upload diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: android-e2e-diagnostics
          path: |
            ~/.maestro/tests/
          if-no-files-found: ignore
          retention-days: 7

      - name: Fail if Maestro tests failed
        if: steps.maestro.outcome == 'failure'
        run: exit 1
