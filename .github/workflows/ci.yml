name: CI

on:
  pull_request:

permissions:
  contents: read
  pull-requests: write

jobs:
  format:
    name: Check Formatting (cargo fmt)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt

      - uses: swatinem/rust-cache@v2

      - name: Check formatting
        run: cargo fmt --check

  clippy:
    name: Lint (cargo clippy)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy

      - uses: swatinem/rust-cache@v2

      - name: Run Clippy (deny warnings)
        run: cargo clippy -- -D warnings

  unit-tests:
    name: Unit Tests & Coverage
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable
        with:
          components: llvm-tools-preview

      - uses: swatinem/rust-cache@v2

      - uses: taiki-e/install-action@cargo-llvm-cov

      - name: Run unit tests with coverage
        id: tests
        run: |
          # Run tests with coverage, capturing both test output and coverage data
          TEST_OUTPUT=$(cargo llvm-cov --bin log-workout --lcov --output-path lcov.info 2>&1)
          echo "$TEST_OUTPUT"
          # Strip Cargo compilation/progress lines; keep only test results
          CLEAN_OUTPUT=$(echo "$TEST_OUTPUT" | grep -Ev '^\s*(Compiling|Checking|Finished|Downloaded|Downloading|Updating|Locking|Fresh)')
          echo "test_output<<EOF" >> "$GITHUB_OUTPUT"
          echo "$CLEAN_OUTPUT" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

      - name: Generate coverage summary
        id: cov
        run: |
          SUMMARY=$(cargo llvm-cov report --summary-only 2>&1)
          echo "summary<<EOF" >> "$GITHUB_OUTPUT"
          echo "$SUMMARY" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"

      - name: Check coverage gate for tested files
        id: gate
        run: |
          # Files that contain unit tests (have #[cfg(test)] blocks)
          TESTED_FILES=$(grep -rl '#\[cfg(test)\]' src/ | sort)
          echo "Tested files:"
          echo "$TESTED_FILES"

          FAIL=0
          GATE_REPORT=""
          for f in $TESTED_FILES; do
            # Extract line coverage for this file from lcov.info
            IN_FILE=0
            LH=0
            LF=0
            while IFS= read -r line; do
              case "$line" in
                SF:*"$f") IN_FILE=1 ;;
                SF:*) IN_FILE=0 ;;
                LH:*) [ "$IN_FILE" = 1 ] && LH="${line#LH:}" ;;
                LF:*) [ "$IN_FILE" = 1 ] && LF="${line#LF:}" ;;
              esac
            done < lcov.info

            if [ "$LF" -gt 0 ]; then
              PCT=$((LH * 100 / LF))
            else
              PCT=0
            fi

            STATUS="âœ…"
            if [ "$PCT" -lt 90 ]; then
              STATUS="âŒ"
              FAIL=1
            fi
            GATE_REPORT="${GATE_REPORT}| ${f} | ${LH}/${LF} | ${PCT}% | ${STATUS} |\n"
            echo "$f: $LH/$LF lines covered ($PCT%)"
          done

          echo "gate_report<<EOF" >> "$GITHUB_OUTPUT"
          echo -e "$GATE_REPORT" >> "$GITHUB_OUTPUT"
          echo "EOF" >> "$GITHUB_OUTPUT"
          echo "gate_failed=$FAIL" >> "$GITHUB_OUTPUT"

      - name: Post unit-test coverage comment
        uses: actions/github-script@v7
        env:
          TEST_OUTPUT: ${{ steps.tests.outputs.test_output }}
          SUMMARY: ${{ steps.cov.outputs.summary }}
          GATE_REPORT: ${{ steps.gate.outputs.gate_report }}
          GATE_FAILED: ${{ steps.gate.outputs.gate_failed }}
        with:
          script: |
            // Access variables via process.env
            const testOutput = process.env.TEST_OUTPUT;
            const summary = process.env.SUMMARY;
            const gateReport = process.env.GATE_REPORT;
            const gateFailed = process.env.GATE_FAILED === '1';
            const gateStatus = gateFailed 
              ? 'âŒ **FAILED** â€” tested files must have â‰¥ 90% coverage' 
              : 'âœ… **PASSED** â€” all tested files have â‰¥ 90% coverage';
            const tag = '';
            const body = `${tag}
            ## ðŸ§ª Unit Tests & Coverage

            ### Test Results
            \`\`\`
            ${testOutput}
            \`\`\`

            ### Coverage Summary
            \`\`\`
            ${summary}
            \`\`\`

            ### Coverage Gate ${gateStatus}
            | File | Lines Hit/Total | Coverage | Status |
            |------|----------------|----------|--------|
            ${gateReport}`;
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            const existing = comments.find(c => c.body.includes(tag));
            if (existing) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existing.id,
                body,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body,
              });
            }

      - name: Upload lcov report
        uses: actions/upload-artifact@v4
        with:
          name: unit-coverage-lcov
          path: lcov.info
          retention-days: 7

      - name: Fail if coverage gate not met
        if: steps.gate.outputs.gate_failed == '1'
        run: |
          echo "::error::Coverage gate failed: one or more tested files have less than 90% line coverage."
          exit 1

  e2e-web:
    name: Web E2E Tests (Maestro)
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: "17"
          distribution: temurin

      - uses: dtolnay/rust-toolchain@stable
        with:
          targets: wasm32-unknown-unknown

      - uses: swatinem/rust-cache@v2

      - name: Install Dioxus CLI
        uses: taiki-e/install-action@v2
        with:
          tool: dioxus-cli

      - name: Build web release
        run: dx build --release --platform web

      - name: Install Maestro
        run: |
          curl -Ls "https://get.maestro.mobile.dev" | bash
          echo "$HOME/.maestro/bin" >> "$GITHUB_PATH"

      - name: Serve PWA and run Maestro web tests
        id: maestro
        continue-on-error: true
        run: |
          # Start a static file server in the background
          python3 -m http.server 8080 -d target/dx/log-workout/release/web/public &
          SERVER_PID=$!
          # Wait for the server to be ready
          for i in $(seq 1 30); do
            curl -sf http://localhost:8080 > /dev/null && break
            sleep 1
          done
          # Maestro's embedded ChromeDriver launches Chrome with a GUI.
          # CI runners have no display, so wrap in xvfb-run to provide a
          # virtual framebuffer and prevent "Chrome instance exited" errors.
          MAESTRO_CLI_NO_ANALYTICS=1 xvfb-run --auto-servernum \
            maestro test --device chromium maestro/web/
          TEST_EXIT=$?
          kill $SERVER_PID 2>/dev/null || true
          exit $TEST_EXIT

      - name: Upload diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: web-e2e-diagnostics
          path: |
            ~/.maestro/tests/
          if-no-files-found: ignore
          retention-days: 7

      - name: Fail if Maestro tests failed
        if: steps.maestro.outcome == 'failure'
        run: exit 1

  e2e-android:
    name: Android E2E Tests (Maestro)
    runs-on: ubuntu-latest
    # Hard cap so the job never hangs if the emulator or Maestro stalls.
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: "17"
          distribution: temurin

      # Build for x86_64 so the APK runs on the CI emulator.
      # Omit --release so Rust compiles with debug_assertions=true. wry 0.53+
      # defaults devtools=true when debug_assertions is on, which calls
      # WebView.setWebContentsDebuggingEnabled(true) â€” enabling the Chrome
      # DevTools Protocol (CDP) socket that Maestro uses to read WebView content.
      - name: Install x86_64 Android Rust target
        uses: dtolnay/rust-toolchain@stable
        with:
          targets: x86_64-linux-android

      - uses: swatinem/rust-cache@v2

      - name: Install Dioxus CLI
        uses: taiki-e/install-action@v2
        with:
          tool: dioxus-cli

      - name: Build x86_64 APK for emulator testing
        run: >-
          dx build
          --platform android
          --target x86_64-linux-android
          --no-default-features
          --features mobile-platform

      - name: Locate x86_64 APK
        id: apk
        run: |
          SRC=$(find target/dx -name "*.apk" -maxdepth 10 | head -1)
          [ -n "$SRC" ] || { echo "::error::No APK found under target/dx"; exit 1; }
          echo "Found APK: $SRC"
          echo "path=$(realpath "$SRC")" >> "$GITHUB_OUTPUT"

      # Enable KVM for hardware-accelerated Android emulation on ubuntu-latest.
      - name: Enable KVM
        run: |
          echo 'KERNEL=="kvm", GROUP="kvm", MODE="0666", OPTIONS+="static_node=kvm"' | sudo tee /etc/udev/rules.d/99-kvm4all.rules
          sudo udevadm control --reload-rules
          sudo udevadm trigger --name-match=kvm

      - name: Install Maestro
        run: |
          curl -Ls "https://get.maestro.mobile.dev" | bash
          echo "$HOME/.maestro/bin" >> "$GITHUB_PATH"

      - name: Run Maestro flows on Android emulator
        id: maestro
        # continue-on-error so the artifact upload step below always runs
        continue-on-error: true
        uses: reactivecircus/android-emulator-runner@v2
        with:
          api-level: 34
          arch: x86_64
          # google_apis provides a Chrome-based WebView with full CDP support.
          # The default AOSP image ships a stripped-down WebView that Maestro
          # cannot inspect, causing all text assertions to time out.
          target: google_apis
          # Disable animations for deterministic, faster tests.
          disable-animations: true
          script: |
            # Force-enable WebView debugging at the OS level so Maestro can
            # inspect the Dioxus WebView via CDP regardless of app flags.
            adb shell settings put global webview_multiprocess 0

            adb install "${{ steps.apk.outputs.path }}"

            # Pre-launch the app and give the WebView time to initialise.
            adb shell monkey -p com.gfaure.logworkout -c android.intent.category.LAUNCHER 1
            sleep 10

            MAESTRO_CLI_NO_ANALYTICS=1 maestro test maestro/android/

      - name: Upload diagnostics
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: android-e2e-diagnostics
          path: |
            ~/.maestro/tests/
          if-no-files-found: ignore
          retention-days: 7

      - name: Fail if Maestro tests failed
        if: steps.maestro.outcome == 'failure'
        run: exit 1
